// Generated by the protocol buffer compiler.  DO NOT EDIT!
// source: tensorflow_serving/apis/inference.proto

#include "tensorflow_serving/apis/inference.pb.h"

#include <algorithm>

#include <google/protobuf/stubs/common.h>
#include <google/protobuf/io/coded_stream.h>
#include <google/protobuf/extension_set.h>
#include <google/protobuf/wire_format_lite_inl.h>
#include <google/protobuf/descriptor.h>
#include <google/protobuf/generated_message_reflection.h>
#include <google/protobuf/reflection_ops.h>
#include <google/protobuf/wire_format.h>
// @@protoc_insertion_point(includes)
#include <google/protobuf/port_def.inc>

extern PROTOBUF_INTERNAL_EXPORT_tensorflow_5fserving_2fapis_2fclassification_2eproto ::google::protobuf::internal::SCCInfo<1> scc_info_ClassificationResult_tensorflow_5fserving_2fapis_2fclassification_2eproto;
extern PROTOBUF_INTERNAL_EXPORT_tensorflow_5fserving_2fapis_2finference_2eproto ::google::protobuf::internal::SCCInfo<1> scc_info_InferenceTask_tensorflow_5fserving_2fapis_2finference_2eproto;
extern PROTOBUF_INTERNAL_EXPORT_tensorflow_5fserving_2fapis_2finference_2eproto ::google::protobuf::internal::SCCInfo<3> scc_info_InferenceResult_tensorflow_5fserving_2fapis_2finference_2eproto;
extern PROTOBUF_INTERNAL_EXPORT_tensorflow_5fserving_2fapis_2finput_2eproto ::google::protobuf::internal::SCCInfo<2> scc_info_Input_tensorflow_5fserving_2fapis_2finput_2eproto;
extern PROTOBUF_INTERNAL_EXPORT_tensorflow_5fserving_2fapis_2fmodel_2eproto ::google::protobuf::internal::SCCInfo<1> scc_info_ModelSpec_tensorflow_5fserving_2fapis_2fmodel_2eproto;
extern PROTOBUF_INTERNAL_EXPORT_tensorflow_5fserving_2fapis_2fregression_2eproto ::google::protobuf::internal::SCCInfo<1> scc_info_RegressionResult_tensorflow_5fserving_2fapis_2fregression_2eproto;
namespace tensorflow {
namespace serving {
class InferenceTaskDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<InferenceTask> _instance;
} _InferenceTask_default_instance_;
class InferenceResultDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<InferenceResult> _instance;
  const ::tensorflow::serving::ClassificationResult* classification_result_;
  const ::tensorflow::serving::RegressionResult* regression_result_;
} _InferenceResult_default_instance_;
class MultiInferenceRequestDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<MultiInferenceRequest> _instance;
} _MultiInferenceRequest_default_instance_;
class MultiInferenceResponseDefaultTypeInternal {
 public:
  ::google::protobuf::internal::ExplicitlyConstructed<MultiInferenceResponse> _instance;
} _MultiInferenceResponse_default_instance_;
}  // namespace serving
}  // namespace tensorflow
static void InitDefaultsInferenceTask_tensorflow_5fserving_2fapis_2finference_2eproto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::tensorflow::serving::_InferenceTask_default_instance_;
    new (ptr) ::tensorflow::serving::InferenceTask();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::tensorflow::serving::InferenceTask::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<1> scc_info_InferenceTask_tensorflow_5fserving_2fapis_2finference_2eproto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 1, InitDefaultsInferenceTask_tensorflow_5fserving_2fapis_2finference_2eproto}, {
      &scc_info_ModelSpec_tensorflow_5fserving_2fapis_2fmodel_2eproto.base,}};

static void InitDefaultsInferenceResult_tensorflow_5fserving_2fapis_2finference_2eproto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::tensorflow::serving::_InferenceResult_default_instance_;
    new (ptr) ::tensorflow::serving::InferenceResult();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::tensorflow::serving::InferenceResult::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<3> scc_info_InferenceResult_tensorflow_5fserving_2fapis_2finference_2eproto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 3, InitDefaultsInferenceResult_tensorflow_5fserving_2fapis_2finference_2eproto}, {
      &scc_info_ModelSpec_tensorflow_5fserving_2fapis_2fmodel_2eproto.base,
      &scc_info_ClassificationResult_tensorflow_5fserving_2fapis_2fclassification_2eproto.base,
      &scc_info_RegressionResult_tensorflow_5fserving_2fapis_2fregression_2eproto.base,}};

static void InitDefaultsMultiInferenceRequest_tensorflow_5fserving_2fapis_2finference_2eproto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::tensorflow::serving::_MultiInferenceRequest_default_instance_;
    new (ptr) ::tensorflow::serving::MultiInferenceRequest();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::tensorflow::serving::MultiInferenceRequest::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<2> scc_info_MultiInferenceRequest_tensorflow_5fserving_2fapis_2finference_2eproto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 2, InitDefaultsMultiInferenceRequest_tensorflow_5fserving_2fapis_2finference_2eproto}, {
      &scc_info_InferenceTask_tensorflow_5fserving_2fapis_2finference_2eproto.base,
      &scc_info_Input_tensorflow_5fserving_2fapis_2finput_2eproto.base,}};

static void InitDefaultsMultiInferenceResponse_tensorflow_5fserving_2fapis_2finference_2eproto() {
  GOOGLE_PROTOBUF_VERIFY_VERSION;

  {
    void* ptr = &::tensorflow::serving::_MultiInferenceResponse_default_instance_;
    new (ptr) ::tensorflow::serving::MultiInferenceResponse();
    ::google::protobuf::internal::OnShutdownDestroyMessage(ptr);
  }
  ::tensorflow::serving::MultiInferenceResponse::InitAsDefaultInstance();
}

::google::protobuf::internal::SCCInfo<1> scc_info_MultiInferenceResponse_tensorflow_5fserving_2fapis_2finference_2eproto =
    {{ATOMIC_VAR_INIT(::google::protobuf::internal::SCCInfoBase::kUninitialized), 1, InitDefaultsMultiInferenceResponse_tensorflow_5fserving_2fapis_2finference_2eproto}, {
      &scc_info_InferenceResult_tensorflow_5fserving_2fapis_2finference_2eproto.base,}};

void InitDefaults_tensorflow_5fserving_2fapis_2finference_2eproto() {
  ::google::protobuf::internal::InitSCC(&scc_info_InferenceTask_tensorflow_5fserving_2fapis_2finference_2eproto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_InferenceResult_tensorflow_5fserving_2fapis_2finference_2eproto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_MultiInferenceRequest_tensorflow_5fserving_2fapis_2finference_2eproto.base);
  ::google::protobuf::internal::InitSCC(&scc_info_MultiInferenceResponse_tensorflow_5fserving_2fapis_2finference_2eproto.base);
}

::google::protobuf::Metadata file_level_metadata_tensorflow_5fserving_2fapis_2finference_2eproto[4];
constexpr ::google::protobuf::EnumDescriptor const** file_level_enum_descriptors_tensorflow_5fserving_2fapis_2finference_2eproto = nullptr;
constexpr ::google::protobuf::ServiceDescriptor const** file_level_service_descriptors_tensorflow_5fserving_2fapis_2finference_2eproto = nullptr;

const ::google::protobuf::uint32 TableStruct_tensorflow_5fserving_2fapis_2finference_2eproto::offsets[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) = {
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceTask, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceTask, model_spec_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceTask, method_name_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceResult, _internal_metadata_),
  ~0u,  // no _extensions_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceResult, _oneof_case_[0]),
  ~0u,  // no _weak_field_map_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceResult, model_spec_),
  offsetof(::tensorflow::serving::InferenceResultDefaultTypeInternal, classification_result_),
  offsetof(::tensorflow::serving::InferenceResultDefaultTypeInternal, regression_result_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::InferenceResult, result_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::MultiInferenceRequest, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::MultiInferenceRequest, tasks_),
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::MultiInferenceRequest, input_),
  ~0u,  // no _has_bits_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::MultiInferenceResponse, _internal_metadata_),
  ~0u,  // no _extensions_
  ~0u,  // no _oneof_case_
  ~0u,  // no _weak_field_map_
  PROTOBUF_FIELD_OFFSET(::tensorflow::serving::MultiInferenceResponse, results_),
};
static const ::google::protobuf::internal::MigrationSchema schemas[] PROTOBUF_SECTION_VARIABLE(protodesc_cold) = {
  { 0, -1, sizeof(::tensorflow::serving::InferenceTask)},
  { 7, -1, sizeof(::tensorflow::serving::InferenceResult)},
  { 16, -1, sizeof(::tensorflow::serving::MultiInferenceRequest)},
  { 23, -1, sizeof(::tensorflow::serving::MultiInferenceResponse)},
};

static ::google::protobuf::Message const * const file_default_instances[] = {
  reinterpret_cast<const ::google::protobuf::Message*>(&::tensorflow::serving::_InferenceTask_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::tensorflow::serving::_InferenceResult_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::tensorflow::serving::_MultiInferenceRequest_default_instance_),
  reinterpret_cast<const ::google::protobuf::Message*>(&::tensorflow::serving::_MultiInferenceResponse_default_instance_),
};

::google::protobuf::internal::AssignDescriptorsTable assign_descriptors_table_tensorflow_5fserving_2fapis_2finference_2eproto = {
  {}, AddDescriptors_tensorflow_5fserving_2fapis_2finference_2eproto, "tensorflow_serving/apis/inference.proto", schemas,
  file_default_instances, TableStruct_tensorflow_5fserving_2fapis_2finference_2eproto::offsets,
  file_level_metadata_tensorflow_5fserving_2fapis_2finference_2eproto, 4, file_level_enum_descriptors_tensorflow_5fserving_2fapis_2finference_2eproto, file_level_service_descriptors_tensorflow_5fserving_2fapis_2finference_2eproto,
};

const char descriptor_table_protodef_tensorflow_5fserving_2fapis_2finference_2eproto[] =
  "\n\'tensorflow_serving/apis/inference.prot"
  "o\022\022tensorflow.serving\032,tensorflow_servin"
  "g/apis/classification.proto\032#tensorflow_"
  "serving/apis/input.proto\032#tensorflow_ser"
  "ving/apis/model.proto\032(tensorflow_servin"
  "g/apis/regression.proto\"W\n\rInferenceTask"
  "\0221\n\nmodel_spec\030\001 \001(\0132\035.tensorflow.servin"
  "g.ModelSpec\022\023\n\013method_name\030\002 \001(\t\"\334\001\n\017Inf"
  "erenceResult\0221\n\nmodel_spec\030\001 \001(\0132\035.tenso"
  "rflow.serving.ModelSpec\022I\n\025classificatio"
  "n_result\030\002 \001(\0132(.tensorflow.serving.Clas"
  "sificationResultH\000\022A\n\021regression_result\030"
  "\003 \001(\0132$.tensorflow.serving.RegressionRes"
  "ultH\000B\010\n\006result\"s\n\025MultiInferenceRequest"
  "\0220\n\005tasks\030\001 \003(\0132!.tensorflow.serving.Inf"
  "erenceTask\022(\n\005input\030\002 \001(\0132\031.tensorflow.s"
  "erving.Input\"N\n\026MultiInferenceResponse\0224"
  "\n\007results\030\001 \003(\0132#.tensorflow.serving.Inf"
  "erenceResultB\003\370\001\001b\006proto3"
  ;
::google::protobuf::internal::DescriptorTable descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto = {
  false, InitDefaults_tensorflow_5fserving_2fapis_2finference_2eproto, 
  descriptor_table_protodef_tensorflow_5fserving_2fapis_2finference_2eproto,
  "tensorflow_serving/apis/inference.proto", &assign_descriptors_table_tensorflow_5fserving_2fapis_2finference_2eproto, 745,
};

void AddDescriptors_tensorflow_5fserving_2fapis_2finference_2eproto() {
  static constexpr ::google::protobuf::internal::InitFunc deps[4] =
  {
    ::AddDescriptors_tensorflow_5fserving_2fapis_2fclassification_2eproto,
    ::AddDescriptors_tensorflow_5fserving_2fapis_2finput_2eproto,
    ::AddDescriptors_tensorflow_5fserving_2fapis_2fmodel_2eproto,
    ::AddDescriptors_tensorflow_5fserving_2fapis_2fregression_2eproto,
  };
 ::google::protobuf::internal::AddDescriptors(&descriptor_table_tensorflow_5fserving_2fapis_2finference_2eproto, deps, 4);
}

// Force running AddDescriptors() at dynamic initialization time.
static bool dynamic_init_dummy_tensorflow_5fserving_2fapis_2finference_2eproto = []() { AddDescriptors_tensorflow_5fserving_2fapis_2finference_2eproto(); return true; }();
namespace tensorflow {
namespace serving {

// ===================================================================

void InferenceTask::InitAsDefaultInstance() {
  ::tensorflow::serving::_InferenceTask_default_instance_._instance.get_mutable()->model_spec_ = const_cast< ::tensorflow::serving::ModelSpec*>(
      ::tensorflow::serving::ModelSpec::internal_default_instance());
}
class InferenceTask::HasBitSetters {
 public:
  static const ::tensorflow::serving::ModelSpec& model_spec(const InferenceTask* msg);
};

const ::tensorflow::serving::ModelSpec&
InferenceTask::HasBitSetters::model_spec(const InferenceTask* msg) {
  return *msg->model_spec_;
}
void InferenceTask::unsafe_arena_set_allocated_model_spec(
    ::tensorflow::serving::ModelSpec* model_spec) {
  if (GetArenaNoVirtual() == nullptr) {
    delete model_spec_;
  }
  model_spec_ = model_spec;
  if (model_spec) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.InferenceTask.model_spec)
}
void InferenceTask::clear_model_spec() {
  if (GetArenaNoVirtual() == nullptr && model_spec_ != nullptr) {
    delete model_spec_;
  }
  model_spec_ = nullptr;
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int InferenceTask::kModelSpecFieldNumber;
const int InferenceTask::kMethodNameFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

InferenceTask::InferenceTask()
  : ::google::protobuf::Message(), _internal_metadata_(nullptr) {
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.InferenceTask)
}
InferenceTask::InferenceTask(::google::protobuf::Arena* arena)
  : ::google::protobuf::Message(),
  _internal_metadata_(arena) {
  SharedCtor();
  RegisterArenaDtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.InferenceTask)
}
InferenceTask::InferenceTask(const InferenceTask& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(nullptr) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  method_name_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (from.method_name().size() > 0) {
    method_name_.Set(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), from.method_name(),
      GetArenaNoVirtual());
  }
  if (from.has_model_spec()) {
    model_spec_ = new ::tensorflow::serving::ModelSpec(*from.model_spec_);
  } else {
    model_spec_ = nullptr;
  }
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.InferenceTask)
}

void InferenceTask::SharedCtor() {
  ::google::protobuf::internal::InitSCC(
      &scc_info_InferenceTask_tensorflow_5fserving_2fapis_2finference_2eproto.base);
  method_name_.UnsafeSetDefault(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  model_spec_ = nullptr;
}

InferenceTask::~InferenceTask() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.InferenceTask)
  SharedDtor();
}

void InferenceTask::SharedDtor() {
  GOOGLE_DCHECK(GetArenaNoVirtual() == nullptr);
  method_name_.DestroyNoArena(&::google::protobuf::internal::GetEmptyStringAlreadyInited());
  if (this != internal_default_instance()) delete model_spec_;
}

void InferenceTask::ArenaDtor(void* object) {
  InferenceTask* _this = reinterpret_cast< InferenceTask* >(object);
  (void)_this;
}
void InferenceTask::RegisterArenaDtor(::google::protobuf::Arena*) {
}
void InferenceTask::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const InferenceTask& InferenceTask::default_instance() {
  ::google::protobuf::internal::InitSCC(&::scc_info_InferenceTask_tensorflow_5fserving_2fapis_2finference_2eproto.base);
  return *internal_default_instance();
}


void InferenceTask::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.InferenceTask)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  method_name_.ClearToEmpty(&::google::protobuf::internal::GetEmptyStringAlreadyInited(), GetArenaNoVirtual());
  if (GetArenaNoVirtual() == nullptr && model_spec_ != nullptr) {
    delete model_spec_;
  }
  model_spec_ = nullptr;
  _internal_metadata_.Clear();
}

#if GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER
const char* InferenceTask::_InternalParse(const char* begin, const char* end, void* object,
                  ::google::protobuf::internal::ParseContext* ctx) {
  auto msg = static_cast<InferenceTask*>(object);
  ::google::protobuf::int32 size; (void)size;
  int depth; (void)depth;
  ::google::protobuf::uint32 tag;
  ::google::protobuf::internal::ParseFunc parser_till_end; (void)parser_till_end;
  auto ptr = begin;
  while (ptr < end) {
    ptr = ::google::protobuf::io::Parse32(ptr, &tag);
    GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
    switch (tag >> 3) {
      // .tensorflow.serving.ModelSpec model_spec = 1;
      case 1: {
        if (static_cast<::google::protobuf::uint8>(tag) != 10) goto handle_unusual;
        ptr = ::google::protobuf::io::ReadSize(ptr, &size);
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
        parser_till_end = ::tensorflow::serving::ModelSpec::_InternalParse;
        object = msg->mutable_model_spec();
        if (size > end - ptr) goto len_delim_till_end;
        ptr += size;
        GOOGLE_PROTOBUF_PARSER_ASSERT(ctx->ParseExactRange(
            {parser_till_end, object}, ptr - size, ptr));
        break;
      }
      // string method_name = 2;
      case 2: {
        if (static_cast<::google::protobuf::uint8>(tag) != 18) goto handle_unusual;
        ptr = ::google::protobuf::io::ReadSize(ptr, &size);
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
        ctx->extra_parse_data().SetFieldName("tensorflow.serving.InferenceTask.method_name");
        object = msg->mutable_method_name();
        if (size > end - ptr + ::google::protobuf::internal::ParseContext::kSlopBytes) {
          parser_till_end = ::google::protobuf::internal::GreedyStringParserUTF8;
          goto string_till_end;
        }
        GOOGLE_PROTOBUF_PARSER_ASSERT(::google::protobuf::internal::StringCheckUTF8(ptr, size, ctx));
        ::google::protobuf::internal::InlineGreedyStringParser(object, ptr, size, ctx);
        ptr += size;
        break;
      }
      default: {
      handle_unusual:
        if ((tag & 7) == 4 || tag == 0) {
          ctx->EndGroup(tag);
          return ptr;
        }
        auto res = UnknownFieldParse(tag, {_InternalParse, msg},
          ptr, end, msg->_internal_metadata_.mutable_unknown_fields(), ctx);
        ptr = res.first;
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr != nullptr);
        if (res.second) return ptr;
      }
    }  // switch
  }  // while
  return ptr;
string_till_end:
  static_cast<::std::string*>(object)->clear();
  static_cast<::std::string*>(object)->reserve(size);
  goto len_delim_till_end;
len_delim_till_end:
  return ctx->StoreAndTailCall(ptr, end, {_InternalParse, msg},
                               {parser_till_end, object}, size);
}
#else  // GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER
bool InferenceTask::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!PROTOBUF_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.InferenceTask)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .tensorflow.serving.ModelSpec model_spec = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) == (10 & 0xFF)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_model_spec()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // string method_name = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) == (18 & 0xFF)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadString(
                input, this->mutable_method_name()));
          DO_(::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
            this->method_name().data(), static_cast<int>(this->method_name().length()),
            ::google::protobuf::internal::WireFormatLite::PARSE,
            "tensorflow.serving.InferenceTask.method_name"));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.InferenceTask)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.InferenceTask)
  return false;
#undef DO_
}
#endif  // GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER

void InferenceTask::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.InferenceTask)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->has_model_spec()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, HasBitSetters::model_spec(this), output);
  }

  // string method_name = 2;
  if (this->method_name().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->method_name().data(), static_cast<int>(this->method_name().length()),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "tensorflow.serving.InferenceTask.method_name");
    ::google::protobuf::internal::WireFormatLite::WriteStringMaybeAliased(
      2, this->method_name(), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:tensorflow.serving.InferenceTask)
}

::google::protobuf::uint8* InferenceTask::InternalSerializeWithCachedSizesToArray(
    ::google::protobuf::uint8* target) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.InferenceTask)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->has_model_spec()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, HasBitSetters::model_spec(this), target);
  }

  // string method_name = 2;
  if (this->method_name().size() > 0) {
    ::google::protobuf::internal::WireFormatLite::VerifyUtf8String(
      this->method_name().data(), static_cast<int>(this->method_name().length()),
      ::google::protobuf::internal::WireFormatLite::SERIALIZE,
      "tensorflow.serving.InferenceTask.method_name");
    target =
      ::google::protobuf::internal::WireFormatLite::WriteStringToArray(
        2, this->method_name(), target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.InferenceTask)
  return target;
}

size_t InferenceTask::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.InferenceTask)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // string method_name = 2;
  if (this->method_name().size() > 0) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::StringSize(
        this->method_name());
  }

  // .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->has_model_spec()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *model_spec_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void InferenceTask::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.InferenceTask)
  GOOGLE_DCHECK_NE(&from, this);
  const InferenceTask* source =
      ::google::protobuf::DynamicCastToGenerated<InferenceTask>(
          &from);
  if (source == nullptr) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.InferenceTask)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.InferenceTask)
    MergeFrom(*source);
  }
}

void InferenceTask::MergeFrom(const InferenceTask& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.InferenceTask)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.method_name().size() > 0) {
    set_method_name(from.method_name());
  }
  if (from.has_model_spec()) {
    mutable_model_spec()->::tensorflow::serving::ModelSpec::MergeFrom(from.model_spec());
  }
}

void InferenceTask::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.InferenceTask)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void InferenceTask::CopyFrom(const InferenceTask& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.InferenceTask)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool InferenceTask::IsInitialized() const {
  return true;
}

void InferenceTask::Swap(InferenceTask* other) {
  if (other == this) return;
  if (GetArenaNoVirtual() == other->GetArenaNoVirtual()) {
    InternalSwap(other);
  } else {
    InferenceTask* temp = New(GetArenaNoVirtual());
    temp->MergeFrom(*other);
    other->CopyFrom(*this);
    InternalSwap(temp);
    if (GetArenaNoVirtual() == nullptr) {
      delete temp;
    }
  }
}
void InferenceTask::UnsafeArenaSwap(InferenceTask* other) {
  if (other == this) return;
  GOOGLE_DCHECK(GetArenaNoVirtual() == other->GetArenaNoVirtual());
  InternalSwap(other);
}
void InferenceTask::InternalSwap(InferenceTask* other) {
  using std::swap;
  _internal_metadata_.Swap(&other->_internal_metadata_);
  method_name_.Swap(&other->method_name_, &::google::protobuf::internal::GetEmptyStringAlreadyInited(),
    GetArenaNoVirtual());
  swap(model_spec_, other->model_spec_);
}

::google::protobuf::Metadata InferenceTask::GetMetadata() const {
  ::google::protobuf::internal::AssignDescriptors(&::assign_descriptors_table_tensorflow_5fserving_2fapis_2finference_2eproto);
  return ::file_level_metadata_tensorflow_5fserving_2fapis_2finference_2eproto[kIndexInFileMessages];
}


// ===================================================================

void InferenceResult::InitAsDefaultInstance() {
  ::tensorflow::serving::_InferenceResult_default_instance_._instance.get_mutable()->model_spec_ = const_cast< ::tensorflow::serving::ModelSpec*>(
      ::tensorflow::serving::ModelSpec::internal_default_instance());
  ::tensorflow::serving::_InferenceResult_default_instance_.classification_result_ = const_cast< ::tensorflow::serving::ClassificationResult*>(
      ::tensorflow::serving::ClassificationResult::internal_default_instance());
  ::tensorflow::serving::_InferenceResult_default_instance_.regression_result_ = const_cast< ::tensorflow::serving::RegressionResult*>(
      ::tensorflow::serving::RegressionResult::internal_default_instance());
}
class InferenceResult::HasBitSetters {
 public:
  static const ::tensorflow::serving::ModelSpec& model_spec(const InferenceResult* msg);
  static const ::tensorflow::serving::ClassificationResult& classification_result(const InferenceResult* msg);
  static const ::tensorflow::serving::RegressionResult& regression_result(const InferenceResult* msg);
};

const ::tensorflow::serving::ModelSpec&
InferenceResult::HasBitSetters::model_spec(const InferenceResult* msg) {
  return *msg->model_spec_;
}
const ::tensorflow::serving::ClassificationResult&
InferenceResult::HasBitSetters::classification_result(const InferenceResult* msg) {
  return *msg->result_.classification_result_;
}
const ::tensorflow::serving::RegressionResult&
InferenceResult::HasBitSetters::regression_result(const InferenceResult* msg) {
  return *msg->result_.regression_result_;
}
void InferenceResult::unsafe_arena_set_allocated_model_spec(
    ::tensorflow::serving::ModelSpec* model_spec) {
  if (GetArenaNoVirtual() == nullptr) {
    delete model_spec_;
  }
  model_spec_ = model_spec;
  if (model_spec) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.InferenceResult.model_spec)
}
void InferenceResult::clear_model_spec() {
  if (GetArenaNoVirtual() == nullptr && model_spec_ != nullptr) {
    delete model_spec_;
  }
  model_spec_ = nullptr;
}
void InferenceResult::set_allocated_classification_result(::tensorflow::serving::ClassificationResult* classification_result) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  clear_result();
  if (classification_result) {
    ::google::protobuf::Arena* submessage_arena =
      reinterpret_cast<::google::protobuf::MessageLite*>(classification_result)->GetArena();
    if (message_arena != submessage_arena) {
      classification_result = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, classification_result, submessage_arena);
    }
    set_has_classification_result();
    result_.classification_result_ = classification_result;
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.InferenceResult.classification_result)
}
void InferenceResult::clear_classification_result() {
  if (has_classification_result()) {
    if (GetArenaNoVirtual() == nullptr) {
      delete result_.classification_result_;
    }
    clear_has_result();
  }
}
void InferenceResult::set_allocated_regression_result(::tensorflow::serving::RegressionResult* regression_result) {
  ::google::protobuf::Arena* message_arena = GetArenaNoVirtual();
  clear_result();
  if (regression_result) {
    ::google::protobuf::Arena* submessage_arena =
      reinterpret_cast<::google::protobuf::MessageLite*>(regression_result)->GetArena();
    if (message_arena != submessage_arena) {
      regression_result = ::google::protobuf::internal::GetOwnedMessage(
          message_arena, regression_result, submessage_arena);
    }
    set_has_regression_result();
    result_.regression_result_ = regression_result;
  }
  // @@protoc_insertion_point(field_set_allocated:tensorflow.serving.InferenceResult.regression_result)
}
void InferenceResult::clear_regression_result() {
  if (has_regression_result()) {
    if (GetArenaNoVirtual() == nullptr) {
      delete result_.regression_result_;
    }
    clear_has_result();
  }
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int InferenceResult::kModelSpecFieldNumber;
const int InferenceResult::kClassificationResultFieldNumber;
const int InferenceResult::kRegressionResultFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

InferenceResult::InferenceResult()
  : ::google::protobuf::Message(), _internal_metadata_(nullptr) {
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.InferenceResult)
}
InferenceResult::InferenceResult(::google::protobuf::Arena* arena)
  : ::google::protobuf::Message(),
  _internal_metadata_(arena) {
  SharedCtor();
  RegisterArenaDtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.InferenceResult)
}
InferenceResult::InferenceResult(const InferenceResult& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(nullptr) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_model_spec()) {
    model_spec_ = new ::tensorflow::serving::ModelSpec(*from.model_spec_);
  } else {
    model_spec_ = nullptr;
  }
  clear_has_result();
  switch (from.result_case()) {
    case kClassificationResult: {
      mutable_classification_result()->::tensorflow::serving::ClassificationResult::MergeFrom(from.classification_result());
      break;
    }
    case kRegressionResult: {
      mutable_regression_result()->::tensorflow::serving::RegressionResult::MergeFrom(from.regression_result());
      break;
    }
    case RESULT_NOT_SET: {
      break;
    }
  }
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.InferenceResult)
}

void InferenceResult::SharedCtor() {
  ::google::protobuf::internal::InitSCC(
      &scc_info_InferenceResult_tensorflow_5fserving_2fapis_2finference_2eproto.base);
  model_spec_ = nullptr;
  clear_has_result();
}

InferenceResult::~InferenceResult() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.InferenceResult)
  SharedDtor();
}

void InferenceResult::SharedDtor() {
  GOOGLE_DCHECK(GetArenaNoVirtual() == nullptr);
  if (this != internal_default_instance()) delete model_spec_;
  if (has_result()) {
    clear_result();
  }
}

void InferenceResult::ArenaDtor(void* object) {
  InferenceResult* _this = reinterpret_cast< InferenceResult* >(object);
  (void)_this;
}
void InferenceResult::RegisterArenaDtor(::google::protobuf::Arena*) {
}
void InferenceResult::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const InferenceResult& InferenceResult::default_instance() {
  ::google::protobuf::internal::InitSCC(&::scc_info_InferenceResult_tensorflow_5fserving_2fapis_2finference_2eproto.base);
  return *internal_default_instance();
}


void InferenceResult::clear_result() {
// @@protoc_insertion_point(one_of_clear_start:tensorflow.serving.InferenceResult)
  switch (result_case()) {
    case kClassificationResult: {
      if (GetArenaNoVirtual() == nullptr) {
        delete result_.classification_result_;
      }
      break;
    }
    case kRegressionResult: {
      if (GetArenaNoVirtual() == nullptr) {
        delete result_.regression_result_;
      }
      break;
    }
    case RESULT_NOT_SET: {
      break;
    }
  }
  _oneof_case_[0] = RESULT_NOT_SET;
}


void InferenceResult::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.InferenceResult)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  if (GetArenaNoVirtual() == nullptr && model_spec_ != nullptr) {
    delete model_spec_;
  }
  model_spec_ = nullptr;
  clear_result();
  _internal_metadata_.Clear();
}

#if GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER
const char* InferenceResult::_InternalParse(const char* begin, const char* end, void* object,
                  ::google::protobuf::internal::ParseContext* ctx) {
  auto msg = static_cast<InferenceResult*>(object);
  ::google::protobuf::int32 size; (void)size;
  int depth; (void)depth;
  ::google::protobuf::uint32 tag;
  ::google::protobuf::internal::ParseFunc parser_till_end; (void)parser_till_end;
  auto ptr = begin;
  while (ptr < end) {
    ptr = ::google::protobuf::io::Parse32(ptr, &tag);
    GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
    switch (tag >> 3) {
      // .tensorflow.serving.ModelSpec model_spec = 1;
      case 1: {
        if (static_cast<::google::protobuf::uint8>(tag) != 10) goto handle_unusual;
        ptr = ::google::protobuf::io::ReadSize(ptr, &size);
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
        parser_till_end = ::tensorflow::serving::ModelSpec::_InternalParse;
        object = msg->mutable_model_spec();
        if (size > end - ptr) goto len_delim_till_end;
        ptr += size;
        GOOGLE_PROTOBUF_PARSER_ASSERT(ctx->ParseExactRange(
            {parser_till_end, object}, ptr - size, ptr));
        break;
      }
      // .tensorflow.serving.ClassificationResult classification_result = 2;
      case 2: {
        if (static_cast<::google::protobuf::uint8>(tag) != 18) goto handle_unusual;
        ptr = ::google::protobuf::io::ReadSize(ptr, &size);
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
        parser_till_end = ::tensorflow::serving::ClassificationResult::_InternalParse;
        object = msg->mutable_classification_result();
        if (size > end - ptr) goto len_delim_till_end;
        ptr += size;
        GOOGLE_PROTOBUF_PARSER_ASSERT(ctx->ParseExactRange(
            {parser_till_end, object}, ptr - size, ptr));
        break;
      }
      // .tensorflow.serving.RegressionResult regression_result = 3;
      case 3: {
        if (static_cast<::google::protobuf::uint8>(tag) != 26) goto handle_unusual;
        ptr = ::google::protobuf::io::ReadSize(ptr, &size);
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
        parser_till_end = ::tensorflow::serving::RegressionResult::_InternalParse;
        object = msg->mutable_regression_result();
        if (size > end - ptr) goto len_delim_till_end;
        ptr += size;
        GOOGLE_PROTOBUF_PARSER_ASSERT(ctx->ParseExactRange(
            {parser_till_end, object}, ptr - size, ptr));
        break;
      }
      default: {
      handle_unusual:
        if ((tag & 7) == 4 || tag == 0) {
          ctx->EndGroup(tag);
          return ptr;
        }
        auto res = UnknownFieldParse(tag, {_InternalParse, msg},
          ptr, end, msg->_internal_metadata_.mutable_unknown_fields(), ctx);
        ptr = res.first;
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr != nullptr);
        if (res.second) return ptr;
      }
    }  // switch
  }  // while
  return ptr;
len_delim_till_end:
  return ctx->StoreAndTailCall(ptr, end, {_InternalParse, msg},
                               {parser_till_end, object}, size);
}
#else  // GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER
bool InferenceResult::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!PROTOBUF_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.InferenceResult)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // .tensorflow.serving.ModelSpec model_spec = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) == (10 & 0xFF)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_model_spec()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .tensorflow.serving.ClassificationResult classification_result = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) == (18 & 0xFF)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_classification_result()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .tensorflow.serving.RegressionResult regression_result = 3;
      case 3: {
        if (static_cast< ::google::protobuf::uint8>(tag) == (26 & 0xFF)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_regression_result()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.InferenceResult)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.InferenceResult)
  return false;
#undef DO_
}
#endif  // GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER

void InferenceResult::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.InferenceResult)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->has_model_spec()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1, HasBitSetters::model_spec(this), output);
  }

  // .tensorflow.serving.ClassificationResult classification_result = 2;
  if (has_classification_result()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      2, HasBitSetters::classification_result(this), output);
  }

  // .tensorflow.serving.RegressionResult regression_result = 3;
  if (has_regression_result()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      3, HasBitSetters::regression_result(this), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:tensorflow.serving.InferenceResult)
}

::google::protobuf::uint8* InferenceResult::InternalSerializeWithCachedSizesToArray(
    ::google::protobuf::uint8* target) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.InferenceResult)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->has_model_spec()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, HasBitSetters::model_spec(this), target);
  }

  // .tensorflow.serving.ClassificationResult classification_result = 2;
  if (has_classification_result()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        2, HasBitSetters::classification_result(this), target);
  }

  // .tensorflow.serving.RegressionResult regression_result = 3;
  if (has_regression_result()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        3, HasBitSetters::regression_result(this), target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.InferenceResult)
  return target;
}

size_t InferenceResult::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.InferenceResult)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // .tensorflow.serving.ModelSpec model_spec = 1;
  if (this->has_model_spec()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *model_spec_);
  }

  switch (result_case()) {
    // .tensorflow.serving.ClassificationResult classification_result = 2;
    case kClassificationResult: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          *result_.classification_result_);
      break;
    }
    // .tensorflow.serving.RegressionResult regression_result = 3;
    case kRegressionResult: {
      total_size += 1 +
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          *result_.regression_result_);
      break;
    }
    case RESULT_NOT_SET: {
      break;
    }
  }
  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void InferenceResult::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.InferenceResult)
  GOOGLE_DCHECK_NE(&from, this);
  const InferenceResult* source =
      ::google::protobuf::DynamicCastToGenerated<InferenceResult>(
          &from);
  if (source == nullptr) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.InferenceResult)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.InferenceResult)
    MergeFrom(*source);
  }
}

void InferenceResult::MergeFrom(const InferenceResult& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.InferenceResult)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  if (from.has_model_spec()) {
    mutable_model_spec()->::tensorflow::serving::ModelSpec::MergeFrom(from.model_spec());
  }
  switch (from.result_case()) {
    case kClassificationResult: {
      mutable_classification_result()->::tensorflow::serving::ClassificationResult::MergeFrom(from.classification_result());
      break;
    }
    case kRegressionResult: {
      mutable_regression_result()->::tensorflow::serving::RegressionResult::MergeFrom(from.regression_result());
      break;
    }
    case RESULT_NOT_SET: {
      break;
    }
  }
}

void InferenceResult::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.InferenceResult)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void InferenceResult::CopyFrom(const InferenceResult& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.InferenceResult)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool InferenceResult::IsInitialized() const {
  return true;
}

void InferenceResult::Swap(InferenceResult* other) {
  if (other == this) return;
  if (GetArenaNoVirtual() == other->GetArenaNoVirtual()) {
    InternalSwap(other);
  } else {
    InferenceResult* temp = New(GetArenaNoVirtual());
    temp->MergeFrom(*other);
    other->CopyFrom(*this);
    InternalSwap(temp);
    if (GetArenaNoVirtual() == nullptr) {
      delete temp;
    }
  }
}
void InferenceResult::UnsafeArenaSwap(InferenceResult* other) {
  if (other == this) return;
  GOOGLE_DCHECK(GetArenaNoVirtual() == other->GetArenaNoVirtual());
  InternalSwap(other);
}
void InferenceResult::InternalSwap(InferenceResult* other) {
  using std::swap;
  _internal_metadata_.Swap(&other->_internal_metadata_);
  swap(model_spec_, other->model_spec_);
  swap(result_, other->result_);
  swap(_oneof_case_[0], other->_oneof_case_[0]);
}

::google::protobuf::Metadata InferenceResult::GetMetadata() const {
  ::google::protobuf::internal::AssignDescriptors(&::assign_descriptors_table_tensorflow_5fserving_2fapis_2finference_2eproto);
  return ::file_level_metadata_tensorflow_5fserving_2fapis_2finference_2eproto[kIndexInFileMessages];
}


// ===================================================================

void MultiInferenceRequest::InitAsDefaultInstance() {
  ::tensorflow::serving::_MultiInferenceRequest_default_instance_._instance.get_mutable()->input_ = const_cast< ::tensorflow::serving::Input*>(
      ::tensorflow::serving::Input::internal_default_instance());
}
class MultiInferenceRequest::HasBitSetters {
 public:
  static const ::tensorflow::serving::Input& input(const MultiInferenceRequest* msg);
};

const ::tensorflow::serving::Input&
MultiInferenceRequest::HasBitSetters::input(const MultiInferenceRequest* msg) {
  return *msg->input_;
}
void MultiInferenceRequest::unsafe_arena_set_allocated_input(
    ::tensorflow::serving::Input* input) {
  if (GetArenaNoVirtual() == nullptr) {
    delete input_;
  }
  input_ = input;
  if (input) {
    
  } else {
    
  }
  // @@protoc_insertion_point(field_unsafe_arena_set_allocated:tensorflow.serving.MultiInferenceRequest.input)
}
void MultiInferenceRequest::clear_input() {
  if (GetArenaNoVirtual() == nullptr && input_ != nullptr) {
    delete input_;
  }
  input_ = nullptr;
}
#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int MultiInferenceRequest::kTasksFieldNumber;
const int MultiInferenceRequest::kInputFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MultiInferenceRequest::MultiInferenceRequest()
  : ::google::protobuf::Message(), _internal_metadata_(nullptr) {
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.MultiInferenceRequest)
}
MultiInferenceRequest::MultiInferenceRequest(::google::protobuf::Arena* arena)
  : ::google::protobuf::Message(),
  _internal_metadata_(arena),
  tasks_(arena) {
  SharedCtor();
  RegisterArenaDtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.MultiInferenceRequest)
}
MultiInferenceRequest::MultiInferenceRequest(const MultiInferenceRequest& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(nullptr),
      tasks_(from.tasks_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  if (from.has_input()) {
    input_ = new ::tensorflow::serving::Input(*from.input_);
  } else {
    input_ = nullptr;
  }
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.MultiInferenceRequest)
}

void MultiInferenceRequest::SharedCtor() {
  ::google::protobuf::internal::InitSCC(
      &scc_info_MultiInferenceRequest_tensorflow_5fserving_2fapis_2finference_2eproto.base);
  input_ = nullptr;
}

MultiInferenceRequest::~MultiInferenceRequest() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.MultiInferenceRequest)
  SharedDtor();
}

void MultiInferenceRequest::SharedDtor() {
  GOOGLE_DCHECK(GetArenaNoVirtual() == nullptr);
  if (this != internal_default_instance()) delete input_;
}

void MultiInferenceRequest::ArenaDtor(void* object) {
  MultiInferenceRequest* _this = reinterpret_cast< MultiInferenceRequest* >(object);
  (void)_this;
}
void MultiInferenceRequest::RegisterArenaDtor(::google::protobuf::Arena*) {
}
void MultiInferenceRequest::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const MultiInferenceRequest& MultiInferenceRequest::default_instance() {
  ::google::protobuf::internal::InitSCC(&::scc_info_MultiInferenceRequest_tensorflow_5fserving_2fapis_2finference_2eproto.base);
  return *internal_default_instance();
}


void MultiInferenceRequest::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.MultiInferenceRequest)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  tasks_.Clear();
  if (GetArenaNoVirtual() == nullptr && input_ != nullptr) {
    delete input_;
  }
  input_ = nullptr;
  _internal_metadata_.Clear();
}

#if GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER
const char* MultiInferenceRequest::_InternalParse(const char* begin, const char* end, void* object,
                  ::google::protobuf::internal::ParseContext* ctx) {
  auto msg = static_cast<MultiInferenceRequest*>(object);
  ::google::protobuf::int32 size; (void)size;
  int depth; (void)depth;
  ::google::protobuf::uint32 tag;
  ::google::protobuf::internal::ParseFunc parser_till_end; (void)parser_till_end;
  auto ptr = begin;
  while (ptr < end) {
    ptr = ::google::protobuf::io::Parse32(ptr, &tag);
    GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
    switch (tag >> 3) {
      // repeated .tensorflow.serving.InferenceTask tasks = 1;
      case 1: {
        if (static_cast<::google::protobuf::uint8>(tag) != 10) goto handle_unusual;
        do {
          ptr = ::google::protobuf::io::ReadSize(ptr, &size);
          GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
          parser_till_end = ::tensorflow::serving::InferenceTask::_InternalParse;
          object = msg->add_tasks();
          if (size > end - ptr) goto len_delim_till_end;
          ptr += size;
          GOOGLE_PROTOBUF_PARSER_ASSERT(ctx->ParseExactRange(
              {parser_till_end, object}, ptr - size, ptr));
          if (ptr >= end) break;
        } while ((::google::protobuf::io::UnalignedLoad<::google::protobuf::uint64>(ptr) & 255) == 10 && (ptr += 1));
        break;
      }
      // .tensorflow.serving.Input input = 2;
      case 2: {
        if (static_cast<::google::protobuf::uint8>(tag) != 18) goto handle_unusual;
        ptr = ::google::protobuf::io::ReadSize(ptr, &size);
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
        parser_till_end = ::tensorflow::serving::Input::_InternalParse;
        object = msg->mutable_input();
        if (size > end - ptr) goto len_delim_till_end;
        ptr += size;
        GOOGLE_PROTOBUF_PARSER_ASSERT(ctx->ParseExactRange(
            {parser_till_end, object}, ptr - size, ptr));
        break;
      }
      default: {
      handle_unusual:
        if ((tag & 7) == 4 || tag == 0) {
          ctx->EndGroup(tag);
          return ptr;
        }
        auto res = UnknownFieldParse(tag, {_InternalParse, msg},
          ptr, end, msg->_internal_metadata_.mutable_unknown_fields(), ctx);
        ptr = res.first;
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr != nullptr);
        if (res.second) return ptr;
      }
    }  // switch
  }  // while
  return ptr;
len_delim_till_end:
  return ctx->StoreAndTailCall(ptr, end, {_InternalParse, msg},
                               {parser_till_end, object}, size);
}
#else  // GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER
bool MultiInferenceRequest::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!PROTOBUF_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.MultiInferenceRequest)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .tensorflow.serving.InferenceTask tasks = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) == (10 & 0xFF)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
                input, add_tasks()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      // .tensorflow.serving.Input input = 2;
      case 2: {
        if (static_cast< ::google::protobuf::uint8>(tag) == (18 & 0xFF)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
               input, mutable_input()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.MultiInferenceRequest)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.MultiInferenceRequest)
  return false;
#undef DO_
}
#endif  // GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER

void MultiInferenceRequest::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.MultiInferenceRequest)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .tensorflow.serving.InferenceTask tasks = 1;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->tasks_size()); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1,
      this->tasks(static_cast<int>(i)),
      output);
  }

  // .tensorflow.serving.Input input = 2;
  if (this->has_input()) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      2, HasBitSetters::input(this), output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:tensorflow.serving.MultiInferenceRequest)
}

::google::protobuf::uint8* MultiInferenceRequest::InternalSerializeWithCachedSizesToArray(
    ::google::protobuf::uint8* target) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.MultiInferenceRequest)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .tensorflow.serving.InferenceTask tasks = 1;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->tasks_size()); i < n; i++) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->tasks(static_cast<int>(i)), target);
  }

  // .tensorflow.serving.Input input = 2;
  if (this->has_input()) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        2, HasBitSetters::input(this), target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.MultiInferenceRequest)
  return target;
}

size_t MultiInferenceRequest::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.MultiInferenceRequest)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // repeated .tensorflow.serving.InferenceTask tasks = 1;
  {
    unsigned int count = static_cast<unsigned int>(this->tasks_size());
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          this->tasks(static_cast<int>(i)));
    }
  }

  // .tensorflow.serving.Input input = 2;
  if (this->has_input()) {
    total_size += 1 +
      ::google::protobuf::internal::WireFormatLite::MessageSize(
        *input_);
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void MultiInferenceRequest::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.MultiInferenceRequest)
  GOOGLE_DCHECK_NE(&from, this);
  const MultiInferenceRequest* source =
      ::google::protobuf::DynamicCastToGenerated<MultiInferenceRequest>(
          &from);
  if (source == nullptr) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.MultiInferenceRequest)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.MultiInferenceRequest)
    MergeFrom(*source);
  }
}

void MultiInferenceRequest::MergeFrom(const MultiInferenceRequest& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.MultiInferenceRequest)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  tasks_.MergeFrom(from.tasks_);
  if (from.has_input()) {
    mutable_input()->::tensorflow::serving::Input::MergeFrom(from.input());
  }
}

void MultiInferenceRequest::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.MultiInferenceRequest)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void MultiInferenceRequest::CopyFrom(const MultiInferenceRequest& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.MultiInferenceRequest)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MultiInferenceRequest::IsInitialized() const {
  return true;
}

void MultiInferenceRequest::Swap(MultiInferenceRequest* other) {
  if (other == this) return;
  if (GetArenaNoVirtual() == other->GetArenaNoVirtual()) {
    InternalSwap(other);
  } else {
    MultiInferenceRequest* temp = New(GetArenaNoVirtual());
    temp->MergeFrom(*other);
    other->CopyFrom(*this);
    InternalSwap(temp);
    if (GetArenaNoVirtual() == nullptr) {
      delete temp;
    }
  }
}
void MultiInferenceRequest::UnsafeArenaSwap(MultiInferenceRequest* other) {
  if (other == this) return;
  GOOGLE_DCHECK(GetArenaNoVirtual() == other->GetArenaNoVirtual());
  InternalSwap(other);
}
void MultiInferenceRequest::InternalSwap(MultiInferenceRequest* other) {
  using std::swap;
  _internal_metadata_.Swap(&other->_internal_metadata_);
  CastToBase(&tasks_)->InternalSwap(CastToBase(&other->tasks_));
  swap(input_, other->input_);
}

::google::protobuf::Metadata MultiInferenceRequest::GetMetadata() const {
  ::google::protobuf::internal::AssignDescriptors(&::assign_descriptors_table_tensorflow_5fserving_2fapis_2finference_2eproto);
  return ::file_level_metadata_tensorflow_5fserving_2fapis_2finference_2eproto[kIndexInFileMessages];
}


// ===================================================================

void MultiInferenceResponse::InitAsDefaultInstance() {
}
class MultiInferenceResponse::HasBitSetters {
 public:
};

#if !defined(_MSC_VER) || _MSC_VER >= 1900
const int MultiInferenceResponse::kResultsFieldNumber;
#endif  // !defined(_MSC_VER) || _MSC_VER >= 1900

MultiInferenceResponse::MultiInferenceResponse()
  : ::google::protobuf::Message(), _internal_metadata_(nullptr) {
  SharedCtor();
  // @@protoc_insertion_point(constructor:tensorflow.serving.MultiInferenceResponse)
}
MultiInferenceResponse::MultiInferenceResponse(::google::protobuf::Arena* arena)
  : ::google::protobuf::Message(),
  _internal_metadata_(arena),
  results_(arena) {
  SharedCtor();
  RegisterArenaDtor(arena);
  // @@protoc_insertion_point(arena_constructor:tensorflow.serving.MultiInferenceResponse)
}
MultiInferenceResponse::MultiInferenceResponse(const MultiInferenceResponse& from)
  : ::google::protobuf::Message(),
      _internal_metadata_(nullptr),
      results_(from.results_) {
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  // @@protoc_insertion_point(copy_constructor:tensorflow.serving.MultiInferenceResponse)
}

void MultiInferenceResponse::SharedCtor() {
  ::google::protobuf::internal::InitSCC(
      &scc_info_MultiInferenceResponse_tensorflow_5fserving_2fapis_2finference_2eproto.base);
}

MultiInferenceResponse::~MultiInferenceResponse() {
  // @@protoc_insertion_point(destructor:tensorflow.serving.MultiInferenceResponse)
  SharedDtor();
}

void MultiInferenceResponse::SharedDtor() {
  GOOGLE_DCHECK(GetArenaNoVirtual() == nullptr);
}

void MultiInferenceResponse::ArenaDtor(void* object) {
  MultiInferenceResponse* _this = reinterpret_cast< MultiInferenceResponse* >(object);
  (void)_this;
}
void MultiInferenceResponse::RegisterArenaDtor(::google::protobuf::Arena*) {
}
void MultiInferenceResponse::SetCachedSize(int size) const {
  _cached_size_.Set(size);
}
const MultiInferenceResponse& MultiInferenceResponse::default_instance() {
  ::google::protobuf::internal::InitSCC(&::scc_info_MultiInferenceResponse_tensorflow_5fserving_2fapis_2finference_2eproto.base);
  return *internal_default_instance();
}


void MultiInferenceResponse::Clear() {
// @@protoc_insertion_point(message_clear_start:tensorflow.serving.MultiInferenceResponse)
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  results_.Clear();
  _internal_metadata_.Clear();
}

#if GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER
const char* MultiInferenceResponse::_InternalParse(const char* begin, const char* end, void* object,
                  ::google::protobuf::internal::ParseContext* ctx) {
  auto msg = static_cast<MultiInferenceResponse*>(object);
  ::google::protobuf::int32 size; (void)size;
  int depth; (void)depth;
  ::google::protobuf::uint32 tag;
  ::google::protobuf::internal::ParseFunc parser_till_end; (void)parser_till_end;
  auto ptr = begin;
  while (ptr < end) {
    ptr = ::google::protobuf::io::Parse32(ptr, &tag);
    GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
    switch (tag >> 3) {
      // repeated .tensorflow.serving.InferenceResult results = 1;
      case 1: {
        if (static_cast<::google::protobuf::uint8>(tag) != 10) goto handle_unusual;
        do {
          ptr = ::google::protobuf::io::ReadSize(ptr, &size);
          GOOGLE_PROTOBUF_PARSER_ASSERT(ptr);
          parser_till_end = ::tensorflow::serving::InferenceResult::_InternalParse;
          object = msg->add_results();
          if (size > end - ptr) goto len_delim_till_end;
          ptr += size;
          GOOGLE_PROTOBUF_PARSER_ASSERT(ctx->ParseExactRange(
              {parser_till_end, object}, ptr - size, ptr));
          if (ptr >= end) break;
        } while ((::google::protobuf::io::UnalignedLoad<::google::protobuf::uint64>(ptr) & 255) == 10 && (ptr += 1));
        break;
      }
      default: {
      handle_unusual:
        if ((tag & 7) == 4 || tag == 0) {
          ctx->EndGroup(tag);
          return ptr;
        }
        auto res = UnknownFieldParse(tag, {_InternalParse, msg},
          ptr, end, msg->_internal_metadata_.mutable_unknown_fields(), ctx);
        ptr = res.first;
        GOOGLE_PROTOBUF_PARSER_ASSERT(ptr != nullptr);
        if (res.second) return ptr;
      }
    }  // switch
  }  // while
  return ptr;
len_delim_till_end:
  return ctx->StoreAndTailCall(ptr, end, {_InternalParse, msg},
                               {parser_till_end, object}, size);
}
#else  // GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER
bool MultiInferenceResponse::MergePartialFromCodedStream(
    ::google::protobuf::io::CodedInputStream* input) {
#define DO_(EXPRESSION) if (!PROTOBUF_PREDICT_TRUE(EXPRESSION)) goto failure
  ::google::protobuf::uint32 tag;
  // @@protoc_insertion_point(parse_start:tensorflow.serving.MultiInferenceResponse)
  for (;;) {
    ::std::pair<::google::protobuf::uint32, bool> p = input->ReadTagWithCutoffNoLastTag(127u);
    tag = p.first;
    if (!p.second) goto handle_unusual;
    switch (::google::protobuf::internal::WireFormatLite::GetTagFieldNumber(tag)) {
      // repeated .tensorflow.serving.InferenceResult results = 1;
      case 1: {
        if (static_cast< ::google::protobuf::uint8>(tag) == (10 & 0xFF)) {
          DO_(::google::protobuf::internal::WireFormatLite::ReadMessage(
                input, add_results()));
        } else {
          goto handle_unusual;
        }
        break;
      }

      default: {
      handle_unusual:
        if (tag == 0) {
          goto success;
        }
        DO_(::google::protobuf::internal::WireFormat::SkipField(
              input, tag, _internal_metadata_.mutable_unknown_fields()));
        break;
      }
    }
  }
success:
  // @@protoc_insertion_point(parse_success:tensorflow.serving.MultiInferenceResponse)
  return true;
failure:
  // @@protoc_insertion_point(parse_failure:tensorflow.serving.MultiInferenceResponse)
  return false;
#undef DO_
}
#endif  // GOOGLE_PROTOBUF_ENABLE_EXPERIMENTAL_PARSER

void MultiInferenceResponse::SerializeWithCachedSizes(
    ::google::protobuf::io::CodedOutputStream* output) const {
  // @@protoc_insertion_point(serialize_start:tensorflow.serving.MultiInferenceResponse)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .tensorflow.serving.InferenceResult results = 1;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->results_size()); i < n; i++) {
    ::google::protobuf::internal::WireFormatLite::WriteMessageMaybeToArray(
      1,
      this->results(static_cast<int>(i)),
      output);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    ::google::protobuf::internal::WireFormat::SerializeUnknownFields(
        _internal_metadata_.unknown_fields(), output);
  }
  // @@protoc_insertion_point(serialize_end:tensorflow.serving.MultiInferenceResponse)
}

::google::protobuf::uint8* MultiInferenceResponse::InternalSerializeWithCachedSizesToArray(
    ::google::protobuf::uint8* target) const {
  // @@protoc_insertion_point(serialize_to_array_start:tensorflow.serving.MultiInferenceResponse)
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  // repeated .tensorflow.serving.InferenceResult results = 1;
  for (unsigned int i = 0,
      n = static_cast<unsigned int>(this->results_size()); i < n; i++) {
    target = ::google::protobuf::internal::WireFormatLite::
      InternalWriteMessageToArray(
        1, this->results(static_cast<int>(i)), target);
  }

  if (_internal_metadata_.have_unknown_fields()) {
    target = ::google::protobuf::internal::WireFormat::SerializeUnknownFieldsToArray(
        _internal_metadata_.unknown_fields(), target);
  }
  // @@protoc_insertion_point(serialize_to_array_end:tensorflow.serving.MultiInferenceResponse)
  return target;
}

size_t MultiInferenceResponse::ByteSizeLong() const {
// @@protoc_insertion_point(message_byte_size_start:tensorflow.serving.MultiInferenceResponse)
  size_t total_size = 0;

  if (_internal_metadata_.have_unknown_fields()) {
    total_size +=
      ::google::protobuf::internal::WireFormat::ComputeUnknownFieldsSize(
        _internal_metadata_.unknown_fields());
  }
  ::google::protobuf::uint32 cached_has_bits = 0;
  // Prevent compiler warnings about cached_has_bits being unused
  (void) cached_has_bits;

  // repeated .tensorflow.serving.InferenceResult results = 1;
  {
    unsigned int count = static_cast<unsigned int>(this->results_size());
    total_size += 1UL * count;
    for (unsigned int i = 0; i < count; i++) {
      total_size +=
        ::google::protobuf::internal::WireFormatLite::MessageSize(
          this->results(static_cast<int>(i)));
    }
  }

  int cached_size = ::google::protobuf::internal::ToCachedSize(total_size);
  SetCachedSize(cached_size);
  return total_size;
}

void MultiInferenceResponse::MergeFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_merge_from_start:tensorflow.serving.MultiInferenceResponse)
  GOOGLE_DCHECK_NE(&from, this);
  const MultiInferenceResponse* source =
      ::google::protobuf::DynamicCastToGenerated<MultiInferenceResponse>(
          &from);
  if (source == nullptr) {
  // @@protoc_insertion_point(generalized_merge_from_cast_fail:tensorflow.serving.MultiInferenceResponse)
    ::google::protobuf::internal::ReflectionOps::Merge(from, this);
  } else {
  // @@protoc_insertion_point(generalized_merge_from_cast_success:tensorflow.serving.MultiInferenceResponse)
    MergeFrom(*source);
  }
}

void MultiInferenceResponse::MergeFrom(const MultiInferenceResponse& from) {
// @@protoc_insertion_point(class_specific_merge_from_start:tensorflow.serving.MultiInferenceResponse)
  GOOGLE_DCHECK_NE(&from, this);
  _internal_metadata_.MergeFrom(from._internal_metadata_);
  ::google::protobuf::uint32 cached_has_bits = 0;
  (void) cached_has_bits;

  results_.MergeFrom(from.results_);
}

void MultiInferenceResponse::CopyFrom(const ::google::protobuf::Message& from) {
// @@protoc_insertion_point(generalized_copy_from_start:tensorflow.serving.MultiInferenceResponse)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

void MultiInferenceResponse::CopyFrom(const MultiInferenceResponse& from) {
// @@protoc_insertion_point(class_specific_copy_from_start:tensorflow.serving.MultiInferenceResponse)
  if (&from == this) return;
  Clear();
  MergeFrom(from);
}

bool MultiInferenceResponse::IsInitialized() const {
  return true;
}

void MultiInferenceResponse::Swap(MultiInferenceResponse* other) {
  if (other == this) return;
  if (GetArenaNoVirtual() == other->GetArenaNoVirtual()) {
    InternalSwap(other);
  } else {
    MultiInferenceResponse* temp = New(GetArenaNoVirtual());
    temp->MergeFrom(*other);
    other->CopyFrom(*this);
    InternalSwap(temp);
    if (GetArenaNoVirtual() == nullptr) {
      delete temp;
    }
  }
}
void MultiInferenceResponse::UnsafeArenaSwap(MultiInferenceResponse* other) {
  if (other == this) return;
  GOOGLE_DCHECK(GetArenaNoVirtual() == other->GetArenaNoVirtual());
  InternalSwap(other);
}
void MultiInferenceResponse::InternalSwap(MultiInferenceResponse* other) {
  using std::swap;
  _internal_metadata_.Swap(&other->_internal_metadata_);
  CastToBase(&results_)->InternalSwap(CastToBase(&other->results_));
}

::google::protobuf::Metadata MultiInferenceResponse::GetMetadata() const {
  ::google::protobuf::internal::AssignDescriptors(&::assign_descriptors_table_tensorflow_5fserving_2fapis_2finference_2eproto);
  return ::file_level_metadata_tensorflow_5fserving_2fapis_2finference_2eproto[kIndexInFileMessages];
}


// @@protoc_insertion_point(namespace_scope)
}  // namespace serving
}  // namespace tensorflow
namespace google {
namespace protobuf {
template<> PROTOBUF_NOINLINE ::tensorflow::serving::InferenceTask* Arena::CreateMaybeMessage< ::tensorflow::serving::InferenceTask >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::serving::InferenceTask >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::serving::InferenceResult* Arena::CreateMaybeMessage< ::tensorflow::serving::InferenceResult >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::serving::InferenceResult >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::serving::MultiInferenceRequest* Arena::CreateMaybeMessage< ::tensorflow::serving::MultiInferenceRequest >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::serving::MultiInferenceRequest >(arena);
}
template<> PROTOBUF_NOINLINE ::tensorflow::serving::MultiInferenceResponse* Arena::CreateMaybeMessage< ::tensorflow::serving::MultiInferenceResponse >(Arena* arena) {
  return Arena::CreateMessageInternal< ::tensorflow::serving::MultiInferenceResponse >(arena);
}
}  // namespace protobuf
}  // namespace google

// @@protoc_insertion_point(global_scope)
#include <google/protobuf/port_undef.inc>
